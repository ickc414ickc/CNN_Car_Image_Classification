{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# local run prepare"
      ],
      "metadata": {
        "id": "StZnM6jgtWGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --user tensorflow-gpu==2.10.0 NumPy==1.23.5 scikit-image==0.19.3 protobuf==3.19.6 googleapis-common-protos==1.60.0 imbalanced-learn==0.11.0"
      ],
      "metadata": {
        "id": "QXWxBx_6nGGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import MaxPool2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import visualkeras\n",
        "\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from lime import submodular_pick\n",
        "\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "8_I5Ix4v4rH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU prepareing"
      ],
      "metadata": {
        "id": "qk1mdWf95FjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "tf.debugging.set_log_device_placement(False)"
      ],
      "metadata": {
        "id": "lHbP3OlClF-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "cmCcChbimAg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"Memory growth enabled for GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found.\")"
      ],
      "metadata": {
        "id": "AaNDxGVulKqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "RT5jEiEh5k0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean gpu memory\n",
        "K.clear_session()"
      ],
      "metadata": {
        "id": "kXYKcdZuSC7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prepareing"
      ],
      "metadata": {
        "id": "cXyC7vjG7MEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "metadata": {
        "id": "2t8DzGjJHapI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('./vehicle_data'))"
      ],
      "metadata": {
        "id": "Y9QipBCyT9cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "p474SPfa30gE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unbalnce data"
      ],
      "metadata": {
        "id": "07GRdJ06oSqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def initialize_environment(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "initialize_environment(seed=42)\n",
        "\n",
        "def prepare_data_generators(dataset_path, img_size=(224, 224), batch_size=32, seed=42):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    train_data = train_datagen.flow_from_directory(\n",
        "        dataset_path,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        subset='training',\n",
        "        class_mode='categorical',\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    validation_data = train_datagen.flow_from_directory(\n",
        "        dataset_path,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        subset='validation',\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    print(\"Class indices:\", train_data.class_indices)\n",
        "    print(\"Number of classes:\", len(validation_data.class_indices))\n",
        "    return train_data, validation_data"
      ],
      "metadata": {
        "id": "uoCIrW5qSIno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data = prepare_data_generators(dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2')\n",
        "class_names = ['Hatchback', 'Other', 'Pickup', 'SUV', 'Seden']\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    images, labels = train_data[i]\n",
        "    plt.imshow(images[i % images.shape[0]])\n",
        "    plt.xlabel(class_names[np.argmax(labels[i % labels.shape[0]])])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fYxYqp-akLS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline model testing (before image segmentation)"
      ],
      "metadata": {
        "id": "JNbkNCW7o9pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def base_model(input_shape=(224, 224, 3), num_classes=5):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6rr0iYA2EVT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_data, validation_data, epochs=10):\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        validation_data=validation_data,\n",
        "        epochs=epochs,\n",
        "    )\n",
        "    return history"
      ],
      "metadata": {
        "id": "sWNc4s-ladKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_metrics(history, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title(loss_title)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(accuracy_title)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "epDfGA9DaiWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_model(seed=42, dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2'):\n",
        "\n",
        "    initialize_environment(seed)\n",
        "\n",
        "    train_data, validation_data = prepare_data_generators(dataset_path, seed=seed)\n",
        "\n",
        "    model = base_model()\n",
        "\n",
        "    history = train_model(model, train_data, validation_data, epochs=10)\n",
        "\n",
        "    plot_training_metrics(history)\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    return model, train_data, validation_data\n",
        "\n",
        "base_model_unbalanced, train_data, validation_data = basic_model(seed=42)"
      ],
      "metadata": {
        "id": "ANeOjDLDLMLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(base_model_unbalanced, legend=True)"
      ],
      "metadata": {
        "id": "3yoNuPn-FPis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### duplicates search"
      ],
      "metadata": {
        "id": "FpoYGJyZolTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.io import imread\n",
        "\n",
        "def pairwise_duplicate_removal(input_folder, output_folder, ssim_threshold=0.9):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for class_folder in os.listdir(input_folder):\n",
        "        class_path = os.path.join(input_folder, class_folder)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing class: {class_folder}\")\n",
        "        output_class_folder = os.path.join(output_folder, class_folder)\n",
        "        os.makedirs(output_class_folder, exist_ok=True)\n",
        "\n",
        "        images = os.listdir(class_path)\n",
        "        num_images = len(images)\n",
        "        to_keep = set(range(num_images))\n",
        "\n",
        "        for i in range(num_images):\n",
        "            if i not in to_keep:\n",
        "                continue\n",
        "\n",
        "            img1_path = os.path.join(class_path, images[i])\n",
        "            img1 = imread(img1_path, as_gray=True)\n",
        "\n",
        "            for j in range(i + 1, num_images):\n",
        "                if j not in to_keep:\n",
        "                    continue\n",
        "\n",
        "                img2_path = os.path.join(class_path, images[j])\n",
        "                img2 = imread(img2_path, as_gray=True)\n",
        "\n",
        "                if img1.shape != img2.shape:\n",
        "                    min_shape = np.minimum(img1.shape, img2.shape)\n",
        "                    img1 = img1[:min_shape[0], :min_shape[1]]\n",
        "                    img2 = img2[:min_shape[0], :min_shape[1]]\n",
        "\n",
        "                similarity = ssim(img1, img2)\n",
        "                if similarity > ssim_threshold:\n",
        "                    print(f\"Duplicate found: {images[i]} and {images[j]} (SSIM: {similarity})\")\n",
        "                    to_keep.discard(j)\n",
        "\n",
        "        for idx in to_keep:\n",
        "            src_path = os.path.join(class_path, images[idx])\n",
        "            dst_path = os.path.join(output_class_folder, images[idx])\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(\"Duplicate removal complete.\")\n",
        "\n",
        "\n",
        "pairwise_duplicate_removal(\n",
        "    input_folder='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2',\n",
        "    output_folder='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 cleaned',\n",
        "    ssim_threshold=0.80\n",
        ")\n",
        "\n",
        "print(\"Duplicates found:\")\n",
        "for duplicate in duplicates:\n",
        "    print(duplicate)"
      ],
      "metadata": {
        "id": "xLY1KsRWnz6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_model(seed=42, dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 cleaned'):\n",
        "\n",
        "    initialize_environment(seed)\n",
        "\n",
        "    train_data, validation_data = prepare_data_generators(dataset_path, seed=seed)\n",
        "\n",
        "    model = base_model()\n",
        "\n",
        "    history = train_model(model, train_data, validation_data, epochs=10)\n",
        "\n",
        "    plot_training_metrics(history)\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    return model, train_data, validation_data\n",
        "\n",
        "base_model_unbalanced, train_data, validation_data = basic_model(seed=42)"
      ],
      "metadata": {
        "id": "MrKw2z_pzngo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lime_pipeline(model, image_paths, img_size=(224, 224), title=\"LIME Results\", num_features=5, num_samples=5000):\n",
        "\n",
        "    def preprocess_image(img_path):\n",
        "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "        return np.expand_dims(img_array, axis=0), img_array\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    n_images = len(image_paths)\n",
        "    cols = 5\n",
        "    rows = (n_images + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        preprocessed_img, original_img = preprocess_image(img_path)\n",
        "\n",
        "        prediction = model.predict(preprocessed_img)\n",
        "        predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "        print(f\"DEBUG: Prediction for image {idx+1}: {prediction}, Predicted class: {predicted_class}\")  # Debugging line\n",
        "\n",
        "        explanation = explainer.explain_instance(\n",
        "            original_img,\n",
        "            model.predict,\n",
        "            top_labels=1,\n",
        "            hide_color=0,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            explanation.top_labels[0],\n",
        "            positive_only=True,\n",
        "            num_features=num_features,\n",
        "            hide_rest=False\n",
        "        )\n",
        "\n",
        "        if idx < len(axes):\n",
        "            axes[idx].imshow(mark_boundaries(temp, mask))\n",
        "            axes[idx].axis('off')\n",
        "            axes[idx].set_title(f\"Predicted: Class {predicted_class}\")\n",
        "\n",
        "    for i in range(len(image_paths), len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PKByHlqHdiRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = base_model_unbalanced\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"Base Model (Data Cleaned) LIME Results\")"
      ],
      "metadata": {
        "id": "befJc-F0dnfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance data"
      ],
      "metadata": {
        "id": "CWrWzYSTo38v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import shutil\n",
        "import Augmentor"
      ],
      "metadata": {
        "id": "6ZhIz6kJQxeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset(\n",
        "    input_dataset_path,\n",
        "    output_dataset_path,\n",
        "    target_images_per_class=5000,\n",
        "    augment_output_name=\"output_\"\n",
        "):\n",
        "\n",
        "    print(\"Copying original images...\")\n",
        "    os.makedirs(output_dataset_path, exist_ok=True)\n",
        "    for cls in os.listdir(input_dataset_path):\n",
        "        class_dir = os.path.join(input_dataset_path, cls)\n",
        "        augmented_class_dir = os.path.join(output_dataset_path, cls)\n",
        "\n",
        "        os.makedirs(augmented_class_dir, exist_ok=True)\n",
        "\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            src = os.path.join(class_dir, img_file)\n",
        "            dst = os.path.join(augmented_class_dir, img_file)\n",
        "            if not os.path.exists(dst):\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "    print(\"Original images copied successfully.\")\n",
        "\n",
        "    for cls in os.listdir(output_dataset_path):\n",
        "        class_dir = os.path.join(output_dataset_path, cls)\n",
        "\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        num_images = len(os.listdir(class_dir))\n",
        "        num_to_generate = target_images_per_class - num_images\n",
        "\n",
        "        if num_to_generate <= 0:\n",
        "            print(f\"{cls} already has sufficient images ({num_images}). No augmentation needed.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Augmenting {cls}: Generating {num_to_generate} new images.\")\n",
        "\n",
        "        p = Augmentor.Pipeline(source_directory=class_dir, output_directory=\".\")\n",
        "\n",
        "        # augmentation operations\n",
        "        p.rotate(probability=0.7, max_left_rotation=15, max_right_rotation=15)\n",
        "        p.flip_left_right(probability=0.5)\n",
        "        p.zoom_random(probability=0.5, percentage_area=0.8)\n",
        "        p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
        "        p.random_brightness(probability=0.5, min_factor=0.8, max_factor=1.2)\n",
        "        p.random_distortion(probability=0.8, grid_width=4, grid_height=4, magnitude=8)\n",
        "        p.sample(num_to_generate)\n",
        "\n",
        "        for img_file in os.listdir(\".\"):\n",
        "            if img_file.startswith(augment_output_name) and img_file.endswith((\".jpg\", \".png\")):\n",
        "                shutil.move(img_file, class_dir)\n",
        "\n",
        "        print(f\"Finished augmenting {cls}. Total images: {len(os.listdir(class_dir))}\")"
      ],
      "metadata": {
        "id": "Tzu7MWyAiDzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test after 2500 data blance"
      ],
      "metadata": {
        "id": "zm4ql8VTgFZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment_dataset(\n",
        "    input_dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 cleaned',\n",
        "    output_dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 balance 2500 cleaned',\n",
        "    target_images_per_class=2500,\n",
        "    augment_output_name=\"output_\"\n",
        ")"
      ],
      "metadata": {
        "id": "-qu_eQlhiaCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model_2500(seed=42, dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 balance 2500 cleaned'):\n",
        "\n",
        "    initialize_environment(seed)\n",
        "\n",
        "    train_data, validation_data = prepare_data_generators(dataset_path, seed=seed)\n",
        "\n",
        "    model = base_model(input_shape=(224, 224, 3), num_classes=5)\n",
        "\n",
        "    history = train_model(model, train_data, validation_data, epochs=10)\n",
        "\n",
        "    plot_training_metrics(history)\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    return model\n",
        "\n",
        "base_model_balanced_2500 = base_model_2500(seed=42)"
      ],
      "metadata": {
        "id": "4y0rGAyZio1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test after 5000 data blance"
      ],
      "metadata": {
        "id": "f4o40K2wYIoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment_dataset(\n",
        "    input_dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 cleaned',\n",
        "    output_dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 balance 5000 cleaned',\n",
        "    target_images_per_class=5000,\n",
        "    augment_output_name=\"output_\"\n",
        ")"
      ],
      "metadata": {
        "id": "8CADJ_oihZ3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model_5000(seed=42, dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 balance 5000 cleaned'):\n",
        "\n",
        "    initialize_environment(seed)\n",
        "\n",
        "    train_data, validation_data = prepare_data_generators(dataset_path, seed=seed)\n",
        "\n",
        "    model = base_model(input_shape=(224, 224, 3), num_classes=5)\n",
        "\n",
        "    history = train_model(model, train_data, validation_data, epochs=10)\n",
        "\n",
        "    plot_training_metrics(history)\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    return model\n",
        "\n",
        "base_model_balanced_5000 = base_model_5000(seed=42)"
      ],
      "metadata": {
        "id": "DrIGGXGdcTLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drop out test"
      ],
      "metadata": {
        "id": "3m0Jmubm1mM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dropout_model(input_shape=(224, 224, 3), num_classes=5, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "jeBq_EZsknWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_model_training_pipeline(seed=42, dataset_path='./vehicle_data/Vehicle Type Image Dataset (Version 2) VTID2 balance 5000 cleaned', epochs=10, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", model_function=None, **kwargs):\n",
        "\n",
        "    initialize_environment(seed)\n",
        "\n",
        "    train_data, validation_data = prepare_data_generators(dataset_path, seed=seed)\n",
        "\n",
        "    model = model_function(input_shape=(224, 224, 3), num_classes=5, **kwargs)\n",
        "\n",
        "    history = train_model(model, train_data, validation_data, epochs=epochs)\n",
        "\n",
        "    plot_training_metrics(history, loss_title=loss_title, accuracy_title=accuracy_title)\n",
        "\n",
        "    K.clear_session()\n",
        "\n",
        "    return model, train_data, validation_data"
      ],
      "metadata": {
        "id": "9UIFKx-oNUpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dropout 0.2"
      ],
      "metadata": {
        "id": "0Xg8epT4nNi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtto5-28ofDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_model_1, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"0.2 Dropout - Training Loss\",\n",
        "    accuracy_title=\"0.2 Dropout - Training Accuracy\",\n",
        "    model_function=dropout_model,\n",
        "    dropout_rate=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "FcGqoS9woezT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dropout 0.3"
      ],
      "metadata": {
        "id": "RS0RXS-dofgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_model_2, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"0.3 Dropout - Training Loss\",\n",
        "    accuracy_title=\"0.3 Dropout - Training Accuracy\",\n",
        "    model_function=dropout_model,\n",
        "    dropout_rate=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "kRONEqMljGQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dropout 0.4"
      ],
      "metadata": {
        "id": "4hbphQnjpBVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_model_3, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"0.4 Dropout - Training Loss\",\n",
        "    accuracy_title=\"0.4 Dropout - Training Accuracy\",\n",
        "    model_function=dropout_model,\n",
        "    dropout_rate=0.4\n",
        ")"
      ],
      "metadata": {
        "id": "v-Y7nfMkpCUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extra layer"
      ],
      "metadata": {
        "id": "ZIjzXLRaAbox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 block"
      ],
      "metadata": {
        "id": "WnWQ4ekXQ2yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_model_3_block(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "rJgDK7q6Cv3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_model_3_block, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"3 block Layer - Training Loss\",\n",
        "    accuracy_title=\"3 block Layer - Training Accuracy\",\n",
        "    model_function=extra_layer_model_3_block\n",
        ")"
      ],
      "metadata": {
        "id": "8TTUPW0vOx4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 block"
      ],
      "metadata": {
        "id": "VBmeBp_5VjD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_model_4_block(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Obwpb28FVnzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_model_4_block_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"4 Block Layer - Training Loss\",\n",
        "    accuracy_title=\"4 Block Layer - Training Accuracy\",\n",
        "    model_function=extra_layer_model_4_block\n",
        ")"
      ],
      "metadata": {
        "id": "W_fQ7a9PVwA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dense layer test"
      ],
      "metadata": {
        "id": "ged4_h12h7Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 dense layer"
      ],
      "metadata": {
        "id": "EpWxaldRmqMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_dense_4_block(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZkYQu1HKiDGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_4_layer, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"4 dense - Training Loss\",\n",
        "    accuracy_title=\"4 dense - Training Accuracy\",\n",
        "    model_function=extra_layer_dense_4_block\n",
        ")"
      ],
      "metadata": {
        "id": "dS0QhASunJFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 dense layer"
      ],
      "metadata": {
        "id": "u5oChZ4hyfsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_dense_5_block(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cO4qwkE_ye9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_instance_5_layer, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"5 dense - Training Loss\",\n",
        "    accuracy_title=\"5 dense - Training Accuracy\",\n",
        "    model_function=extra_layer_dense_5_block\n",
        ")"
      ],
      "metadata": {
        "id": "VIdP0VTSyqdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = extra_layer_dense_instance_5_layer\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"5 Dense Layer LIME Results\")"
      ],
      "metadata": {
        "id": "4V1LLIaIFKDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BatchNormalization"
      ],
      "metadata": {
        "id": "KUelghw3340w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_BatchNormalization(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "pugbg07R4IoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_instance_BatchNormalization, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"BatchNormalization - Training Loss\",\n",
        "    accuracy_title=\"BatchNormalization - Training Accuracy\",\n",
        "    model_function=extra_layer_BatchNormalization\n",
        ")"
      ],
      "metadata": {
        "id": "Qu-BR_N94Oqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = extra_layer_dense_instance_BatchNormalization\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"BatchNormalization LIME Results\")"
      ],
      "metadata": {
        "id": "ckH9SZ48p6Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization"
      ],
      "metadata": {
        "id": "JDzcQPN3AJ2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 0.01"
      ],
      "metadata": {
        "id": "8af92-2_FRRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "VW0LUozPFd9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_dense_with_l2(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_dye2tVEAJLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_l2, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"L2 0.01 - Training Loss\",\n",
        "    accuracy_title=\"L2 0.01 - Training Accuracy\",\n",
        "    model_function=extra_layer_dense_with_l2\n",
        ")"
      ],
      "metadata": {
        "id": "HWKrWlUOFAHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = extra_layer_dense_l2\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"L2 0.01 LIME Results\")"
      ],
      "metadata": {
        "id": "2VhhWyt_T8V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 0.001"
      ],
      "metadata": {
        "id": "Ra-YFSS01uyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_dense_with_l2_1(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "aJtQjytDy3dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_l2_1, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"L2 0.001 - Training Loss\",\n",
        "    accuracy_title=\"L2 0.001 - Training Accuracy\",\n",
        "    model_function=extra_layer_dense_with_l2_1\n",
        ")"
      ],
      "metadata": {
        "id": "YexRSWxUy772"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 0.0001"
      ],
      "metadata": {
        "id": "zX5nQKYz25tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_layer_dense_with_l2_2(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.0001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "-SNoBcGY277t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_dense_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"L2 0.0001 - Training Loss\",\n",
        "    accuracy_title=\"L2 0.0001 - Training Accuracy\",\n",
        "    model_function=extra_layer_dense_with_l2_2\n",
        ")"
      ],
      "metadata": {
        "id": "MD73prlJ3AIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## learning rate"
      ],
      "metadata": {
        "id": "UiJW2pNZKHR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def extra_layer_learning_rate(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "n5s1IL3uKDtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate 0.001"
      ],
      "metadata": {
        "id": "4Ne60MWOMVWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"Learning Rate 0.001 - Training Loss\",\n",
        "    accuracy_title=\"Learning Rate 0.001 - Training Accuracy\",\n",
        "    model_function=extra_layer_learning_rate,\n",
        "    learning_rate=0.001\n",
        ")"
      ],
      "metadata": {
        "id": "8zpjy84pMdPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate 0.0001"
      ],
      "metadata": {
        "id": "kSDm_JnGPYjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"Learning Rate 0.0001 - Training Loss\",\n",
        "    accuracy_title=\"Learning Rate 0.0001 - Training Accuracy\",\n",
        "    model_function=extra_layer_learning_rate,\n",
        "    learning_rate=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "M9LlVPfdMEx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Learning rate"
      ],
      "metadata": {
        "id": "y8CXNorda9uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "def extra_layer_dynamic_learning_rate(input_shape=(224, 224, 3), num_classes=5, initial_learning_rate=0.0001, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GJ4jvS4aa9Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "60hraG6P7Iq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "def train_model(model, train_data, validation_data, epochs=10):\n",
        "    initial_learning_rate = 1e-4\n",
        "    decay_steps = train_data.samples // train_data.batch_size * epochs\n",
        "    lr_schedule = CosineDecay(initial_learning_rate, decay_steps)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        validation_data=validation_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=[reduce_lr]\n",
        "    )\n",
        "    return history"
      ],
      "metadata": {
        "id": "0E2J-cdQjFV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_layer_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"Dynamic Learning Rate - Training Loss\",\n",
        "    accuracy_title=\"Dynamic Learning Rate - Training Accuracy\",\n",
        "    model_function=extra_layer_dynamic_learning_rate\n",
        ")"
      ],
      "metadata": {
        "id": "-9wcDkYLecUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "icjmPvzR62Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD, Nadam, Adam"
      ],
      "metadata": {
        "id": "sNSVPN6y7p1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_addons.optimizers import AdamW"
      ],
      "metadata": {
        "id": "wYTVygI_HVPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ],
      "metadata": {
        "id": "kWgxCdyd7R6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = SGD(learning_rate=0.0001, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BeqGNBMm6_BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"SGD Optimizer - Training Loss\",\n",
        "    accuracy_title=\"SGD Optimizer - Training Accuracy\",\n",
        "    model_function=SGD_model\n",
        ")"
      ],
      "metadata": {
        "id": "Bv5dEIyz7vMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nadam"
      ],
      "metadata": {
        "id": "TXdOBAt78K07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Nadam_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Nadam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "K4RFQmQj8KIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nadam_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"Nadam Optimizer - Training Loss\",\n",
        "    accuracy_title=\"Nadam Optimizer - Training Accuracy\",\n",
        "    model_function=Nadam_model\n",
        ")"
      ],
      "metadata": {
        "id": "QNcZGdnTDZqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdamW"
      ],
      "metadata": {
        "id": "_0zAUm1KDMuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AdamW_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = AdamW(learning_rate=0.0001, weight_decay=0.0001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6lhTPegQDVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AdamW_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"AdamW Optimizer - Training Loss\",\n",
        "    accuracy_title=\"AdamW Optimizer - Training Accuracy\",\n",
        "    model_function=AdamW_model\n",
        ")"
      ],
      "metadata": {
        "id": "YX9cq7AHDpSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling method"
      ],
      "metadata": {
        "id": "CxrBWgG2Mrj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MaxPooling (baseline)"
      ],
      "metadata": {
        "id": "Q5CrydObskXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MaxPooling_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.0001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001,\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rN3RGczUM3LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MaxPooling_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"MaxPooling (baseline) - Training Loss\",\n",
        "    accuracy_title=\"MaxPooling (baseline) - Training Accuracy\",\n",
        "    model_function=MaxPooling_model\n",
        ")"
      ],
      "metadata": {
        "id": "C0sLdiN4N8Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MaxPooling_model_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"MaxPooling (base model) LIME Results\")"
      ],
      "metadata": {
        "id": "QWQbtQjPLbOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AveragePooling"
      ],
      "metadata": {
        "id": "Z_bEhGKGqJdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import AveragePooling2D"
      ],
      "metadata": {
        "id": "1Of8pROLGh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AveragePooling_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "WVqwm--in3Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AveragePooling_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"AveragePooling - Training Loss\",\n",
        "    accuracy_title=\"AveragePooling - Training Accuracy\",\n",
        "    model_function=AveragePooling_model\n",
        ")"
      ],
      "metadata": {
        "id": "DZCo24qsoRWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AveragePooling_model_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"AveragePooling model LIME Results\")"
      ],
      "metadata": {
        "id": "1anQNVHOLrXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GlobalAveragePooling"
      ],
      "metadata": {
        "id": "GkfST4_EqMPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalAveragePooling_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.001,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "--eLnWztrTP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalAveragePooling_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"GlobalAveragePooling - Training Loss\",\n",
        "    accuracy_title=\"GlobalAveragePooling - Training Accuracy\",\n",
        "    model_function=GlobalAveragePooling_model\n",
        ")"
      ],
      "metadata": {
        "id": "hOJVSXkesrjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GlobalAveragePooling_model_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"GlobalAveragePooling model LIME Results\")"
      ],
      "metadata": {
        "id": "ZRTRrMTHLyzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GlobalMaxPooling"
      ],
      "metadata": {
        "id": "GqeDu8vWtuQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMaxPooling_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.001,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "rTGKDTKrtsqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMaxPooling_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"GlobalMaxPooling - Training Loss\",\n",
        "    accuracy_title=\"GlobalMaxPooling - Training Accuracy\",\n",
        "    model_function=GlobalMaxPooling_model\n",
        ")"
      ],
      "metadata": {
        "id": "40ZRZXaet5tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GlobalMaxPooling_model_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"GlobalMaxPooling model Results\")"
      ],
      "metadata": {
        "id": "O15nxe_TL44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kernel size"
      ],
      "metadata": {
        "id": "ZKJSLe0xXiiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### size 3"
      ],
      "metadata": {
        "id": "uEjqNC0LZoo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_size_model_1(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.0001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "SkBTmOGMY4VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_size_model_1_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"kernel size 3 - Training Loss\",\n",
        "    accuracy_title=\"kernel size 3 - Training Accuracy\",\n",
        "    model_function=kernel_size_model_1\n",
        ")"
      ],
      "metadata": {
        "id": "UEFfhbgWZO9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = kernel_size_model_1_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"kernel size 3 model Results\")"
      ],
      "metadata": {
        "id": "IRPHaVbCZm6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### size 5"
      ],
      "metadata": {
        "id": "jKNH4fL-ZsQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_size_model_2(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.0001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "WkxttO6wZuWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_size_model_2_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"kernel size 5 - Training Loss\",\n",
        "    accuracy_title=\"kernel size 5 - Training Accuracy\",\n",
        "    model_function=kernel_size_model_2\n",
        ")"
      ],
      "metadata": {
        "id": "IfEB58x3Z3wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = kernel_size_model_2_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"kernel size 5 model Results\")"
      ],
      "metadata": {
        "id": "XWgMKkmyckUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## padding method"
      ],
      "metadata": {
        "id": "STUfn7zZ1Tz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_padding_model(input_shape=(224, 224, 3), num_classes=5, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\", learning_rate=0.0001):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=3, padding='valid', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "emp2TCp11XnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_padding_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"valid padding - Training Loss\",\n",
        "    accuracy_title=\"valid padding - Training Accuracy\",\n",
        "    model_function=valid_padding_model\n",
        ")"
      ],
      "metadata": {
        "id": "1wHTRDU51sQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = valid_padding_model_instance\n",
        "image_paths = [\"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\\PHOTO_14.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\\PHOTO_11.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\\PHOTO_3.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\\PHOTO_7.jpg\", \"./vehicle_data\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\\PHOTO_17.jpg\"]\n",
        "lime_pipeline(model, image_paths, title=\"valid padding model Results\")"
      ],
      "metadata": {
        "id": "8ZDMIEmR5Fej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final model"
      ],
      "metadata": {
        "id": "lIvDU31BeeuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "def Final_model(input_shape=(224, 224, 3), num_classes=5, initial_learning_rate=0.0001, loss_title=\"Training - Loss Function\", accuracy_title=\"Training - Accuracy\"):\n",
        "    model = Sequential()\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Four convolutional block\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax', dtype='float32'))\n",
        "\n",
        "    # with the initial learning rate\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "p976dwvJesCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_model_instance, train_data, validation_data = extra_layer_model_training_pipeline(\n",
        "    seed=42,\n",
        "    epochs=10,\n",
        "    loss_title=\"Dynamic Learning Rate - Training Loss\",\n",
        "    accuracy_title=\"Dynamic Learning Rate - Training Accuracy\",\n",
        "    model_function=Final_model\n",
        ")"
      ],
      "metadata": {
        "id": "Y7sVGwH2e5cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(Final_model_instance, legend=True)"
      ],
      "metadata": {
        "id": "gV8EkguofBoO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}